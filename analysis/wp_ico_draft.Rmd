---
title: "Does Whitepaper Page Count Affect ICO Valuation?"
author: "Michael Rosenberg and Michael McCaffrey"
date: "11/22/2018"
output: pdf_document
---

```{r setup, include=FALSE}
base_dir <- "~/Dropbox/repos/page_count_crypto_analysis"
knitr::opts_chunk$set(echo = TRUE,
                      root.dir = base_dir)
#packages
library(lubridate)
library(tree)
library(ggplot2)
library(gridExtra)
library(knitr)
library(kableExtra)

#helper constants
pch_lev <- 19
percent_lev <- 100
num_breaks_per_hist <- 5
alpha_lev <- .5
who_am_i <- 24601
num_folds <- 5
block_hexcolor <- "#00BDBB"
```

```{r helper_functions,include=FALSE}
#plotting helpers
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  #adapted from http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

get_xlab_for_hist <- function(ico_df,var_name,var_label,round_lev = 1){
    #Helper function for getting the string for the xlabel of a histogram for
    #the var_name of interest.
    #get statistics
    var_mean <- round(mean(ico_df[,var_name]),round_lev)
    var_min <- round(min(ico_df[,var_name]),round_lev)
    var_max <- round(max(ico_df[,var_name]),round_lev)
    #get strings to add
    mean_str <- paste("Mean =",var_mean,sep = " ")
    range_str <- paste("Range = [",var_min,",",var_max,"]",sep = " ")
    xlab_str <- paste(var_label,
                      "(",
                      mean_str,
                      ",",
                      range_str,
                      ")",
                      sep = " ")
    return(xlab_str)
}

#modeling helpers
get_cross_val_rmse <- function(wp_ico_frame,formula_str,fold_var){
    #helper for getting cross-validated RMSE (in untransformed terms) for our
    #given dataset
    fold_vec <- wp_ico_frame[,fold_var]
    rmse_vec <- c()
    for (fold_num in unique(fold_vec)){
        #filter
        at_fold_num <- (fold_vec == fold_num)
        test_frame <- wp_ico_frame[at_fold_num,]
        train_frame <- wp_ico_frame[!at_fold_num,]
        #then fit
        train_lm <- lm(formula_str,data = train_frame)
        #then test
        test_pred <- predict(train_lm,newdata = test_frame)
        #then transform
        test_nonlogged_pred <- exp(test_pred)
        target_var_name <- strsplit(formula_str," ")[[1]][1]
        test_nonlogged_actuals <- exp(test_frame[,target_var_name])
        #get rmse
        test_rmse <- sqrt(mean((test_nonlogged_actuals - test_nonlogged_pred) ** 2))
        rmse_vec <- c(rmse_vec,test_rmse)
    }
    return(mean(rmse_vec))
}
```

```{r set_up_figure_dir,include=FALSE}
#get base figure directory
base_figure_dir <- paste(base_dir,"figures",sep = "/")
dir.create(base_figure_dir,showWarnings = FALSE)
#get analysis-specific figure directory
analysis_figure_dir <- paste(base_figure_dir,"page_count_analysis",sep = "/")
dir.create(analysis_figure_dir,showWarnings = FALSE)
```

```{r load_data,include=FALSE}
wp_ico_frame <- read.csv(
    paste(base_dir,"data/preprocessed/cleaned_wp_ico.csv",sep = "/"),
    header = TRUE)
```

# Introduction {#introduction}

Within the cryptocurrency markets, there has been a [recent uptick in whitepaper
length](https://www.longhash.com/news/why-ico-white-papers-keep-getting-longer).
Between Q1 2016 and Q4 2018, the word count has increased from around 3000 words
per paper to 9000 words per paper. Many ICO projects slated to launch in 2019
look to be continuing this trend.

We are interested in seeing if this length increase informs any ICO valuation
processes. In particular, does whitepaper length predict higher ICO valuations
by close date?

Our intuition suggests that whitepaper length might be an indication of ICO
complexity. On one hand, this complexity might be a sign of innovative work,
which would imply a higher  **(FILL INFORMATION IN HERE)**

To analyze this question, we collected amount raised as close date per
cryptocurrency via [Coindesk's ICO Tracker](https://www.coindesk.com/ico-tracker).
We then manually looked up each cryptocurrency's whitepaper and identified the
page count on those papers. Due to the work-intensive nature of that manual
process, we decided to solely analyze ICOs between January 2018 and July 2018.
We will discuss the implications of this data subsetting in our [next steps](
#next_steps).

# Data Exploration {#data_exploration}

Within our dataset, there are around `r dim(wp_ico_frame)[1]` ICOs between
January and July of this year. This dataset size is relatively small, which
suggests that we may not be powered to see statistically significant results
with a large feature set. We may be able to do a more in-depth analysis when we
consider earlier years in our [future work](#next_steps).

For our analysis, we will be predicting the amount raised in ICO (in millions)
per cryptocurrency using page count. Let's take a look at our variables of
interest.

```{r plot_amount_raised,echo=FALSE,message=FALSE}
#cleanup from dollar to numeric
wp_ico_frame$amount_raised_m <- as.numeric(gsub('[$,]', '',
                                           wp_ico_frame$Amount.Raised.in.ICO...M.))

#plot
non_transformed_hist <- (
    ggplot(data = wp_ico_frame, aes(amount_raised_m))
        + geom_histogram(col = block_hexcolor,
                         fill = block_hexcolor)
        + xlab(get_xlab_for_hist(wp_ico_frame
                              , "amount_raised_m"
                              , "Amount Raised ($M)"
               )
        )
        + ylab ("Count")
        + ggtitle("Distribution of Amount Raised ($M)")
        + theme_bw()
)
log_transformed_hist <- (
    ggplot(data = wp_ico_frame, aes(log(amount_raised_m)))
        + geom_histogram(col = block_hexcolor,
                         fill = block_hexcolor)
        + xlab("Log-Amount Raised ($M)")
        + ylab ("Count")
        + ggtitle("Distribution of Log-Amount Raised ($M)")
        + theme_bw()
        + scale_x_continuous(breaks = round(seq(min(log(wp_ico_frame$amount_raised_m)), 
                                                max(log(wp_ico_frame$amount_raised_m)),
                                                length.out = num_breaks_per_hist),1))
)
ggsave(paste(analysis_figure_dir,"amount_raised_hist.png",sep = "/"),
       arrangeGrob(non_transformed_hist,log_transformed_hist))
multiplot(non_transformed_hist,log_transformed_hist)
```
_Figure 1: Distribution of Amount Raised in ICO ($M). The regular amount raised
is on the top, while the log-amount raised is on the bottom._

We see that the raw amount raised is very right-skewed (top). This is typical of
financial data; there are many ICOs that have raised relatively little by their
close date and a handful of ICOs that have raised a huge amount of money. For
reference, the median amount raised is around
\$`r round(median(wp_ico_frame$amount_raised_m),1)`M while the max is around
\$`r round(max(wp_ico_frame$amount_raised_m),1)`M. While this is perfectly reasonable
as a financial process, it is often difficult for simple predictive models to
fit right-skewed variables. Because the [natural logarithm](https://en.wikipedia.org/wiki/Logarithm)
of amount raised is much more normally distributed (which tends to be easier to
predict with simple regression methods), we will aim to predict the
log-transformed version of our amount raised in our [methodology](#methodology).

```{r plot_page_count_dist,echo=FALSE,message=FALSE}
page_count_hist <- (
    ggplot(data = wp_ico_frame, aes(Page.count))
        + geom_histogram(col = block_hexcolor,
                         fill = block_hexcolor)
        + xlab(get_xlab_for_hist(wp_ico_frame
                                  , "Page.count"
                                  , "Page Count"
               )
        )
        + ylab ("Count")
        + ggtitle("Distribution of Page Count Across Whitepapers")
        + theme_bw()
        + scale_x_continuous(breaks = round(seq(min(wp_ico_frame$Page.count), 
                                                max(wp_ico_frame$Page.count),
                                                length.out = num_breaks_per_hist),0))
)
ggsave(paste(analysis_figure_dir,"page_count_hist.png",sep = "/"),
       page_count_hist)
page_count_hist
```

_Figure 2: Distribution of Page Count per ICO whitepaper._

Like amount raised, page count is also a right-skewed variable. On average,
ICO whitepapers tend to be around `r round(mean(wp_ico_frame$Page.count),1)`,
but the longest whitepaper in our dataset is around `r round(max(wp_ico_frame$Page.count),1)` pages. Since most simple regression
methods make no normality assumptions about explanatory variables, I am not too
concerned about this. However, the sparsity of the page count distribution above
70 pages suggests that we may not currently be able to make statistically
meaningful statements about very long whitepapers.

```{r prepare_assets_for_bivariate_plot_analysis,echo=FALSE,message=FALSE}
#calculate some cutoff information
test_cutoff <- 54
wp_ico_frame$above_cutoff <- (wp_ico_frame$Page.count >= test_cutoff)
#outlier values
outlier_row <- which.max(wp_ico_frame$Page.count)
outlier_page_count <- wp_ico_frame$Page.count[outlier_row]
outlier_log_amt_raised <- log(wp_ico_frame$amount_raised_m[outlier_row])
offset_x <- 10
offset_y <- 1
#get rid of outlier
outlier_removed_ico_frame <- wp_ico_frame[-outlier_row,]
#get some notes pre- and post-cutoff
mean_log_amt_raised_pre_cutoff <- mean(
                                       log(
                            outlier_removed_ico_frame$amount_raised_m[!outlier_removed_ico_frame$above_cutoff]
                                       )
)
mean_log_amt_raised_post_cutoff <- mean(
                                       log(
                            outlier_removed_ico_frame$amount_raised_m[outlier_removed_ico_frame$above_cutoff]
                                       )
)
mean_amount_lift <- (
    (exp(mean_log_amt_raised_post_cutoff) / exp(mean_log_amt_raised_pre_cutoff)) - 1
)
presentable_lift <- round(mean_amount_lift * percent_lev,2)
#get percentile transformation
#normalize cutoff
test_cutoff_percentile <- (
    (1 - (sum(wp_ico_frame$above_cutoff) / length(wp_ico_frame$above_cutoff))) 
    * percent_lev
)
```

```{r bivariate_plot,echo=FALSE,message=FALSE}
#then plot
log_amt_raised_on_page_count_plot <- (
    ggplot(data = wp_ico_frame,aes(x = Page.count
                                 , y = log(amount_raised_m)
                               )
    )
        + geom_point(col = block_hexcolor
                   , fill = block_hexcolor
                   , alpha = alpha_lev
        )
        #get linear predictor
        + geom_smooth(method = "lm"
                    , se = FALSE
                    , data = outlier_removed_ico_frame
        )
        #get cutoff at test
        + geom_vline(xintercept = test_cutoff
                   , col = "black"
                   , linetype = "dashed"
                   , alpha = alpha_lev - .1)
        #get group means
        + geom_smooth(aes(y = log(amount_raised_m),group = above_cutoff)
                    , formula = y ~ 1
                    , method = "lm"
                    , col = "red"
                    , linetype = "dashed"
                    , se = FALSE
                    , data = outlier_removed_ico_frame
        )
        #point out outlier
        + annotate("text"
                 , x = outlier_page_count - offset_x
                 , y = outlier_log_amt_raised - offset_y
                 , label = "Outlier"
                 , fontface = 2
                 , vjust = 1
                 , hjust = .5
        )
        + annotate("segment"
                 , x = outlier_page_count - offset_x
                 , y = outlier_log_amt_raised - offset_y
                 , xend = outlier_page_count
                 , yend = outlier_log_amt_raised
                 , size = .5
                 , alpha = alpha_lev + .1
                 , arrow = arrow()
        )
        #scale ticks
        + scale_x_continuous(breaks = round(seq(min(wp_ico_frame$Page.count)
                                              , max(wp_ico_frame$Page.count)
                                              , length.out = num_breaks_per_hist
                                            )
                                          , 0
                                      )
        )
        + scale_y_continuous(breaks = round(seq(min(log(wp_ico_frame$amount_raised_m))
                                              , max(log(wp_ico_frame$amount_raised_m))
                                              , length.out = num_breaks_per_hist
                                            )
                                          , 0
                                      )
        )
        #label and theme
        + theme_bw()
        + xlab("Page Count")
        + ylab("Log-Amount Raised ($M)")
        + ggtitle("Log-Amount Raised ($M) on Page Count")
)
ggsave(paste(analysis_figure_dir,"log_amt_raised_on_page_count.png",sep = "/"),
       log_amt_raised_on_page_count_plot)
log_amt_raised_on_page_count_plot
```

_Figure 3: Log-Amount Raised ($M) on Page Count (teal). We have removed the
page count outlier (Page Count = `r max(wp_ico_frame$Page.count)`) from
annotation within the plot. The blue line represents the linear trend for the
core ICO set. The dashed black line represents the page count of
`r test_cutoff`, while the red dashed lines represent the mean log-Amount raised
pre-cutoff and post-cutoff._

When plotting log-amount raised on page count, there is a very clear outlier
at around `r max(wp_ico_frame$Page.count)` pages. Given that the second largest
page count is only around `r max(outlier_removed_ico_frame$Page.count)` pages
and the amount raised at the `r max(wp_ico_frame$Page.count)`-page whitepaper
is very high, I feel uncomfortable interpolating the page count effect within 
this gap. Thus, we are going to remove the `r max(wp_ico_frame$Page.count)`-page
whitepaper from our analysis.

By the linear trend (blue), we see that there is a key positive relationship
between increased page counts and amount raised. However, it is also clear based
on our observations (teal), the noise around the linear trend is non-constant.
In particular, it looks like the variation of log-amount raised decreases after
around `r test_cutoff` pages. This [heteroscedasticity](https://en.wikipedia.org/wiki/Heteroscedasticity)
may violate some of the assumptions around statistically testing the
relationship between log-amount raised and page count. Due to our timeline, we
leave this robustness check outside the scope of this analysis. We will review
the implications in our [next steps](#next_steps).

When analyzing this relationship, we also noticed a clear conditional lift in
log-amount raised at around the `r test_cutoff`-page mark. If we just analyze
mean amount raised pre-cutoff and post-cutoff, we measure around a
`r presentable_lift`\% lift in amount raised. While this measurement will
likely be dampened when controlling for other sources of variation (see
[methodology](#methodology)), this lift seems substantial enough to be
considered as an alternative predictive hypothesis to a linear trend (blue).
While the `r test_cutoff` number is relatively arbitrary, it corresponds with
the `r round(test_cutoff_percentile,0)`th percentile of the page count distribution.
Thus, we will consider a model that represents the page count effect on
log-amount raised as a lift for cryptocurrencies with whitepapers in the 
top `r percent_lev - round(test_cutoff_percentile,0)`\% for page count.

# Methodology {#methodology}

Given the size of our data, we want to limit this initial analysis to using
simple linear regression. Let $Y_i$ be the amount raised (in \$M) for c
ryptocurrency $i$. As discussed
before, $Y_i$ is very right-skewed, which is difficult to predict using simple
regression methods. Based on Figure 1, we think it will be easier to generate
a good fit on $\log(Y_i)$.

Given the manual process of gathering more data per cryptocurrency, we are
planning to focus on the effect of page count on log-amount raised with only
controls on seasonality (i.e. when the ICO closed in 2018). Based on our
data exploration, we will consider 2 models of interest:

1. A linear page count effect model:
    \begin{equation}
    \log(Y_i) \sim Page\_Count_i  + \sum_{t = 1}^7 I(Month\_Of\_Close_i == t).
    \end{equation}
    + $Page\_Count_i$ is the raw page count of cryptocurrency $i$'s whitepaper.
    + $Month\_Of\_Close_i$ indicates the month of the ICO close date for 
        cryptocurrency $i$. Since we are only considering ICOs between January
        and July of 2018, this will be an integer between 1 and 7.
    + $I(Month\_Of\_Close_i == t)$ is an indicator that equals 1 when
        cryptocurrency $i$ closed in month $t$ and 0 when said cryptocurrency
        closed in a month other than $t$. We sum 7 of these indicators for the 7
        months featured in our dataset.
    + These indicator variables are meant to control for seasonality. They
        control for the variation in ICO amount raised from month to month,
        regardless of the page count of cryptocurrency $i$. These may be
        accounting for the effects of speculative hype within the cryptocurrency
        business, which can vary over time.
2. A percentile effect model:
    \begin{equation}
    \log(Y_i) \sim I(Percentile(Page\_Count_i) >= `r round(test_cutoff_percentile)`\%) + \sum_{t = 1}^7 I(Month\_Of\_Close_i == t).
    \end{equation}
    + $Percentile(Page\_Count_i) = x$\% means $x$\% of whitepapers have equal or
        fewer pages than cryptocurrency $i$'s whitepaper. Thus,
        $I(Percentile(Page\_Count_i) >= `r round(test_cutoff_percentile)`\%) = 1$
        for cryptocurrencies that have page counts in the top
        `r percent_lev - round(test_cutoff_percentile)`\% of the page count
        distribution. This is essentially a binary effect that creates a lift
        in log-amount raised for cryptocurrencies in the top 
        `r percent_lev - round(test_cutoff_percentile)`\% for whitepaper length.
        This cutoff was inspired by our exploration in Figure 3.
        
Interpreting our regression for $\log(Y_i)$ is a bit different than for a
regression on $Y_i$. Simply put, our effects are examined as multiplicative
changes on $Y_i$ rather than linear changes to $Y_i$.
Say that for instance, we have fit $\log(Y_i)$ with the first model:

$$f(Page\_Count_i,Month\_of\_Close_i) = \beta_0 + \beta_1 Page\_Count_i + \sum_{t = 1}^7 \delta_t I(Month\_Of\_Close_i == t),$$

Where we predict $\widehat{\log(Y_i)} = f(Page\_Count_i,Month\_of\_Close_i).$ We
can recover our amount raised prediction by exponentiating $\widehat{\log(Y_i)}$:

$$\widehat{Y_i} = e^{\widehat{\log(Y_i)}},$$

where $e$ is the [base of the natural logarithm](https://en.wikipedia.org/wiki/E_(mathematical_constant)).
it's apparent then that we predict $\widehat{Y_i}$ using $e^{f(Page\_Count_i,Month\_of\_Close_i)}.$

This exponentiation makes all of the linear effects in $f(Page\_Count_i,Month\_of\_Close_i)$ turn into multiplicative effects on
$Y_i$. For instance, say I wanted to predict the effect on amount raised when
adding an additional page to a whitepaper. Our prediction for log-amount raised
($\widehat{\log(Y_i)}$) will be

$$f(Page\_Count_i + 1,Month\_Of\_Close_i) = \beta_1 + f(Page\_Count_i,Month\_Of\_Close_i).$$

Thus, our prediction for $\widehat{Y_i}$ will be

$$e^{\beta_1 + f(Page\_Count_i,Month\_Of\_Close_i)}
= e^{\beta_1} \cdot e^{f(Page\_Count_i,Month\_Of\_Close_i)}.$$

Since we originally predicted amount raised with 
$e^{f(Page\_Count_i,Month\_Of\_Close_i)}$, we see that adding an additional
page to a white paper is predicted to multiply amount raised by $e^{\beta_1}$.
Thus, an additional page is predicted to increase amount raised by
$(e^{\beta_1} - 1) \cdot 100$\%.

We will evaluate our models using cross-validated [root mean-squared error (RMSE)](https://en.wikipedia.org/wiki/Mean_squared_error). RMSE is a metric
that measures the average difference between our
amount raised predictions ($\hat{Y_i}$) and their actual values in the dataset 
($Y_i$). Cross-validated RMSE is designed to see how well our model performs on
average for data outside of our training sample (i.e. test data). This gives
us a robustness check on how well our model generalizes to outside data.
We calculate cross-validated RMSE for each of our models the following way:

* Let $D$ be our set of $n$ datapoints ($|D| = n$). We are considering $F$
models to evaluate ($M_1,M_2,...M_F$).
* Randomly partition $D$ into $K$ equally-sized folds ($D_1,D_2,...,D_K$).
    We will iterate through these folds as our test datasets.
* For each model type $f \in [F]$:
    + Set $RMSE\_Set = \emptyset$. We will store out-of-sample RMSE in this set
    for each fold.
    + For each fold $k \in [K]$:
        * Train model $M_f$ using all data besides $D_k$ ($D - D_k$).
        * Predict amount raised ($\hat{Y_i}$) using $M_f$ on $D_k$.
        * Calculate (on $D_k$):
\begin{equation}
Given\_RMSE = \sqrt{\text{AVG}((Y - \hat{Y})^2)}.
\end{equation}
        * Add $Given\_RMSE$ to your $RMSE\_Set$ (i.e. $RMSE\_Set := RMSE\_Set \cup \{Given\_RMSE\}$).
    + Get cross-validated RMSE for model $M_f$ via $CV\_RMSE_f = \text{AVG}(RMSE\_Set)$.

Given the small size of our dataset and standard cross-validation practices, 
we have chosen $K = 5$ folds for our model evaluation process. We will choose
to select and analyze the model that minimizes cross-validated RMSE.

```{r filter_dataset,include=FALSE}
outlier_row <- which.max(wp_ico_frame$Page.count)
filtered_wp_ico_frame <- wp_ico_frame[-outlier_row,]
```

```{r engineer_features_for_method,include=FALSE}
#target
filtered_wp_ico_frame$log_amount_raised_m <- log(filtered_wp_ico_frame$amount_raised_m)
#features
filtered_wp_ico_frame$month_of_close <- month(
                                    as.POSIXct(filtered_wp_ico_frame$ICO.Close.Date,
                                               format = "%m/%d/%y"))
#normalize page count
min_page_count <- min(filtered_wp_ico_frame$Page.count)
max_page_count <- max(filtered_wp_ico_frame$Page.count)
norm_page_count <- ((filtered_wp_ico_frame$Page.count - min_page_count)
                    / (max_page_count - min_page_count))
page_percentile <- norm_page_count * percent_lev
filtered_wp_ico_frame$page_count_percentile <- page_percentile
#transform cutoff
test_cutoff <- 54
norm_test_cutoff <- (
                        ((test_cutoff - min_page_count)
                         / (max_page_count - min_page_count)
                        ) * percent_lev
)
filtered_wp_ico_frame$above_cutoff <- (filtered_wp_ico_frame$page_count_percentile >= norm_test_cutoff)
```


```{r prepare_for_crossval,include=FALSE}
#generate fold var
set.seed(who_am_i)
fold_levels <- 1:num_folds
fold_vec <- rep(fold_levels,length.out = dim(filtered_wp_ico_frame)[1])
#permute
fold_vec <- sample(fold_vec)
filtered_wp_ico_frame$fold <- fold_vec
```

```{r, get_cv_rmse,echo=FALSE,message=FALSE}
linear_formula <- "log_amount_raised_m ~ Page.count + as.factor(month_of_close)"
binary_formula <- "log_amount_raised_m ~ above_cutoff + as.factor(month_of_close)"
linear_cv_rmse <- get_cross_val_rmse(filtered_wp_ico_frame,linear_formula,"fold")
binary_cv_rmse <- get_cross_val_rmse(filtered_wp_ico_frame,binary_formula,"fold")
```

# Results {#results}

We see that the cross-validated RMSE for the linear and percentile effect models
are `r round(linear_cv_rmse,2)` and `r round(binary_cv_rmse,2)` respectively.
While these RMSEs are very close, I will select the linear model
(model 1 in the [Methodology](#methodology) section) since its cross-validated
RMSE is slightly smaller than the percentile model's cross-validated RMSE.

```{r chosen_mod,echo=FALSE,message=FALSE}
final_mod_lm <- lm(log_amount_raised_m ~ Page.count + as.factor(month_of_close)
                 , data = filtered_wp_ico_frame
)
#save model
model_dir <- paste(base_dir,"models",sep = "/")
dir.create(model_dir,showWarnings = FALSE)
saveRDS(final_mod_lm,paste(model_dir,"final_mod_lm.rds",sep = "/"))
```

That being said, this RMSE is concerning from a fit perspective. The linear
model implies that, on average, our model is off by around 
\$`r round(linear_cv_rmse)`M for each cryptocurrency's ICO. This is pretty
severe underfitting of the valuation process, and I think it is worthwhile
to consider a more feature-dense model in our [next steps](#next_steps).

```{r process_summary_table,echo=FALSE,message=FALSE}
round_lev <- 3
#get table
summary_table <- as.data.frame(summary(final_mod_lm)$coefficients)
#clean
summary_table <- round(summary_table,round_lev)
#get rid of t value column
summary_table <- summary_table[,!(colnames(summary_table) %in% c("t value"))]
#get percent change column
summary_table$Percent_Change <- round(
                        (exp(summary_table$Estimate) - 1)
                      , round_lev + 2
)
summary_table$Percent_Change[1] <- NA
#then rename some columns and rows
colnames(summary_table) <- c("Coefficient"
                           , "Std. Error"
                           , "P-Value"
                           , "Percent Change")
rownames(summary_table) <- c("(Intercept)"
                           , "Page Count"
                           , "Month Of Close = 2 (February)"
                           , "Month Of Close = 3 (March)"
                           , "Month Of Close = 4 (April)"
                           , "Month Of Close = 5 (May)"
                           , "Month Of Close = 6 (June)"
                           , "Month Of Close = 7 (July)"
)
#then save
write.csv(summary_table
        , file = paste(base_dir,"data/processed/coef_table.csv",sep = "/")
        , na = "")
```

```{r display_summary_table,echo=FALSE,message=FALSE}
#final processing on percent change
summary_table$`Percent Change` <- paste(summary_table$`Percent Change` * percent_lev
                                      , "%"
                                      , sep = "")
summary_table[1,"Percent Change"] <- NA
#save html version of the table
html_table <- kable_styling(kable(summary_table,"html")
                          , bootstrap_options = c("striped", "hover")
)
cat(html_table,file = paste(analysis_figure_dir,"coef_table.html",sep = "/"))
#then present official coefficient table
kable(summary_table)
```

_Table 1: The coefficient table from our selected regression. "Percent Change"
is the expected percent change in amount raised ($M) implied by the coefficient
estimates._

We see that when we control for seasonality, increasing the length of a
whitepaper by 1 page is predicted to increase amount raised by
around 1\%. This is also very statistically significant, with a p-value below
$.01$. This means there is a statistically significant likelihood that is having
some effect on amount raised. That being said, there are still open questions
on the narrative of the effect. On one end, page count might be simply a form
of obfuscation; there might not be major differences in the qualities of
different cryptocurrencies, but whitepaper length might give an impression of
complexity and due-diligence for an ICO that causes investors to value it
higher. On the other hand, there might be genuine content differences that is
informing both the length of whitepapers and there general valuation (e.g.
new technological breakthroughs, ambitious designs). In this regard, it will
be important to further analyze the language content of these whitepapers in
our [next steps](#next_steps).

While there is varying statistical significance of our month indicators, their
negative coefficients make it clear that there is a general decline in ICO
valuation post-February. It may be the case that enthusiasm
around cryptocurrency has declined over the year, which could be informing lower
ICO valuations post-February.

```{r check_for_multicollinearity,echo=FALSE,message=FALSE}
num_months <- length(unique(filtered_wp_ico_frame$month_of_close))
page_count_on_month_plot <- (
    ggplot(aes(x = month_of_close
             , y = Page.count
           )
         , data = filtered_wp_ico_frame
    )
    #plot with means
    + geom_point(col = block_hexcolor
               , fill = block_hexcolor
               , alpha = alpha_lev
    )
    + stat_summary(fun.y = mean
                 , color = "blue"
                 , geom = "line"
                 , group = 1
    )
    + scale_x_continuous(breaks = 1:num_months)
    + scale_y_continuous(breaks = round(seq(min(filtered_wp_ico_frame$Page.count)
                                          , max(filtered_wp_ico_frame$Page.count)
                                          , length.out = num_breaks_per_hist
                                        )
                                      , 0
                                  )
    )
    #label and theme
    + theme_bw()
    + xlab("Month of Close")
    + ylab ("Page Count")
    + ggtitle("Page Count on Month of Close (2018)")
)
ggsave(paste(analysis_figure_dir,"page_count_on_month_of_close.png",sep = "/"),
       page_count_on_month_plot)
page_count_on_month_plot
```

_Figure 4: Page Count on Month of Close for ICOs in our modeling dataset (teal).
The page count means per month of close are indicated by the blue line._

As a robustness check, we wanted to make sure there was little collinearity
between page count and month of close. If there was, it would make it difficult
to interpret the page count effect on amount raised
when controlling for seasonality. Thankfully,
it looks like we will not have to worry substantially about this issue. Across
the months of our dataset, page count hovers between 32 and 36 pages (blue
line). Since this is very little variation in mean across months, we would
argue that we do not need to be concerned about this multicollinearity when
interpreting the effect of page count on amount raised.

# Next Steps {#next_steps}
